2025-06-01 01:26:49,179 - INFO - PyTorch version 2.7.0 available.
2025-06-01 01:27:02,116 - INFO - Sample train set 1500/5428
2025-06-01 01:27:02,116 - INFO - ... including dev set 500 samples
2025-06-01 01:27:02,116 - INFO - Loading model with FP16...
OurArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
alternate_training=False,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
eos_token=<EOS_TOKEN>,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=1000,
eval_strategy=steps,
eval_use_gather_object=False,
fo_optimizer=sgd,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
hybrid_optimizer=True,
icl_sfc=False,
if_sgd_dropout=False,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
layer_wise_hybrid=True,
learning_rate=0.0001,
length_column_name=length,
linear_probing=False,
load_best_model_at_end=True,
load_bfloat16=False,
load_float16=True,
load_int8=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=result/WIC-deepseek-moe-16b-base-mezo-ft-with-hybrid-opt-last-24layer-sgd-step-1-no-dropout-10000-16-1e-4-1e-7-1e-3-0/runs/Jun01_01-26-50_autodl-container-d2a7408c62-3ca0231e,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lora=False,
lora_alpha=16,
lora_r=8,
lp_early_stopping=False,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
masking_prob=0.99,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=50,
max_steps=10000,
metric_for_best_model=loss,
model_name=deepseek-ai/deepseek-moe-16b-base,
mp_parameters=,
neftune_noise_alpha=None,
no_auto_device=False,
no_cuda=False,
no_eval=False,
no_reparam=True,
non_diff=False,
num_beams=1,
num_dev=500,
num_eval=1000,
num_prefix=5,
num_train=1000,
num_train_epochs=3.0,
num_train_sets=None,
only_predict=False,
only_train_option=True,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=result/WIC-deepseek-moe-16b-base-mezo-ft-with-hybrid-opt-last-24layer-sgd-step-1-no-dropout-10000-16-1e-4-1e-7-1e-3-0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=16,
prediction_loss_only=False,
prefix_init_by_real_act=True,
prefix_tuning=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
result_file=None,
resume_from_checkpoint=None,
run_name=result/WIC-deepseek-moe-16b-base-mezo-ft-with-hybrid-opt-last-24layer-sgd-step-1-no-dropout-10000-16-1e-4-1e-7-1e-3-0,
sampling=False,
save_model=False,
save_on_each_node=False,
save_on_interrupt=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
sfc=False,
sgd_optim_layers=['router', 'gate'],
sgd_optim_step=1,
skip_memory_metrics=True,
tag=mezo-ft-with-hybrid-opt-last-24layer-sgd-step-1-no-dropout-10000-16-1e-4-1e-7-1e-3-0,
task_name=WIC,
temperature=1.0,
tf32=None,
top_k=None,
top_p=0.95,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_as_classification=True,
train_set_seed=0,
trainer=zo,
untie_emb=False,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
verbose=True,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
weight_wise_hybrid=False,
zo_eps=0.001,
zo_learning_rate=1e-07,
)
DeepseekConfig {
  "activation_dropout": 0,
  "architectures": [
    "DeepseekForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0,
  "auto_map": {
    "AutoConfig": "deepseek-ai/deepseek-moe-16b-base--configuration_deepseek.DeepseekConfig",
    "AutoModel": "deepseek-ai/deepseek-moe-16b-base--modeling_deepseek.DeepseekModel",
    "AutoModelForCausalLM": "deepseek-ai/deepseek-moe-16b-base--modeling_deepseek.DeepseekForCausalLM"
  },
  "aux_loss_alpha": 0.001,
  "bos_token_id": 100000,
  "dropout": 0,
  "eos_token_id": 100001,
  "first_k_dense_replace": 1,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 10944,
  "max_position_embeddings": 4096,
  "model_type": "deepseek",
  "moe_intermediate_size": 1408,
  "moe_layer_freq": 1,
  "n_routed_experts": 64,
  "n_shared_experts": 2,
  "norm_topk_prob": false,
  "num_attention_heads": 16,
  "num_experts_per_tok": 6,
  "num_hidden_layers": 28,
  "num_key_value_heads": 16,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000,
  "scoring_func": "softmax",
  "seq_aux": true,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.53.0.dev0",
  "use_cache": true,
  "vocab_size": 102400
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.07s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.06s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.03s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.02s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.02s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.02s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]
2025-06-01 01:27:10,715 - INFO - Done with 8.60s
2025-06-01 01:27:11,256 - INFO - Tokenizing training samples...
2025-06-01 01:27:12,484 - INFO - Done with 1.23s
/root/LoHO/large_models/run.py:494: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `OurTrainer.__init__`. Use `processing_class` instead.
  trainer = OurTrainer(
Traceback (most recent call last):
  File "/root/LoHO/large_models/run.py", line 623, in <module>
    main()
  File "/root/LoHO/large_models/run.py", line 581, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples)
  File "/root/LoHO/large_models/run.py", line 494, in train
    trainer = OurTrainer(
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/transformers/trainer.py", line 622, in __init__
    self._move_model_to_device(model, args.device)
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/transformers/trainer.py", line 905, in _move_model_to_device
    model = model.to(device)
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3848, in to
    return super().to(*args, **kwargs)
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
  File "/root/miniconda3/envs/loho/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 31.48 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 31.47 GiB memory in use. Of the allocated memory 25.98 GiB is allocated by PyTorch, and 5.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
